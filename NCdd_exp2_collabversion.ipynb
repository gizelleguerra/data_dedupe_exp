{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "NCdd_exp2.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4ROXojM3u26"
      },
      "source": [
        "### Record Linkage Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYPlQ4rO3u29"
      },
      "source": [
        "This code demonstrates how to use RecordLink with two comma separated\n",
        "values (CSV) files. We have listings of products from two different\n",
        "online stores. The task is to link products between the datasets.\n",
        "The output will be a CSV with our linkded results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-txHOctO4b5n",
        "outputId": "4d84e662-6326-4585-9402-4cdad23adf12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Install Dedupe.io\n",
        "!pip install dedupe"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dedupe in /usr/local/lib/python3.7/dist-packages (2.0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from dedupe) (3.7.4.3)\n",
            "Requirement already satisfied: dedupe-hcluster in /usr/local/lib/python3.7/dist-packages (from dedupe) (0.3.8)\n",
            "Requirement already satisfied: highered>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from dedupe) (0.2.1)\n",
            "Requirement already satisfied: fastcluster in /usr/local/lib/python3.7/dist-packages (from dedupe) (1.2.4)\n",
            "Requirement already satisfied: rlr>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from dedupe) (2.4.5)\n",
            "Requirement already satisfied: affinegap>=1.3 in /usr/local/lib/python3.7/dist-packages (from dedupe) (1.11)\n",
            "Requirement already satisfied: doublemetaphone in /usr/local/lib/python3.7/dist-packages (from dedupe) (0.1)\n",
            "Requirement already satisfied: zope.index in /usr/local/lib/python3.7/dist-packages (from dedupe) (5.1.0)\n",
            "Requirement already satisfied: categorical-distance>=1.9 in /usr/local/lib/python3.7/dist-packages (from dedupe) (1.9)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.7/dist-packages (from dedupe) (1.21.2)\n",
            "Requirement already satisfied: BTrees>=4.1.4 in /usr/local/lib/python3.7/dist-packages (from dedupe) (4.9.2)\n",
            "Requirement already satisfied: Levenshtein-search in /usr/local/lib/python3.7/dist-packages (from dedupe) (1.4.5)\n",
            "Requirement already satisfied: haversine>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from dedupe) (2.5.1)\n",
            "Requirement already satisfied: dedupe-variable-datetime in /usr/local/lib/python3.7/dist-packages (from dedupe) (0.1.5)\n",
            "Requirement already satisfied: simplecosine>=1.2 in /usr/local/lib/python3.7/dist-packages (from dedupe) (1.2)\n",
            "Requirement already satisfied: zope.interface>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from BTrees>=4.1.4->dedupe) (5.4.0)\n",
            "Requirement already satisfied: persistent>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from BTrees>=4.1.4->dedupe) (4.7.0)\n",
            "Requirement already satisfied: pyhacrf-datamade>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from highered>=0.2.0->dedupe) (0.2.5)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from persistent>=4.1.0->BTrees>=4.1.4->dedupe) (1.14.6)\n",
            "Requirement already satisfied: PyLBFGS>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from pyhacrf-datamade>=0.2.0->highered>=0.2.0->dedupe) (0.2.0.13)\n",
            "Requirement already satisfied: future>=0.14 in /usr/local/lib/python3.7/dist-packages (from rlr>=2.4.3->dedupe) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zope.interface>=5.0.0->BTrees>=4.1.4->dedupe) (57.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->persistent>=4.1.0->BTrees>=4.1.4->dedupe) (2.20)\n",
            "Requirement already satisfied: datetime-distance in /usr/local/lib/python3.7/dist-packages (from dedupe-variable-datetime->dedupe) (0.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from datetime-distance->dedupe-variable-datetime->dedupe) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.6.0->datetime-distance->dedupe-variable-datetime->dedupe) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-awOGeL4qyB",
        "outputId": "fd8ab835-233d-4af9-f3b7-738942ec1e33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.0-py2.py3-none-any.whl (235 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 29.3 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 71 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 81 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 92 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye9DUDdi3u2-"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import re\n",
        "import logging\n",
        "import optparse\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FTN7d7D6KLz"
      },
      "source": [
        "import dedupe"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wY5Udp_4Lyx"
      },
      "source": [
        "from unidecode import unidecode"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkko5P0o4M1a"
      },
      "source": [
        "import collections\n",
        "import itertools"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cse4gzfS3u2_"
      },
      "source": [
        "def preProcess(column):\n",
        "    \"\"\"\n",
        "    Do a little bit of data cleaning with the help of Unidecode and Regex.\n",
        "    Things like casing, extra spaces, quotes and new lines can be ignored.\n",
        "    \"\"\"\n",
        "\n",
        "    column = unidecode(column)\n",
        "    column = re.sub('\\n', ' ', column)\n",
        "    column = re.sub('-', '', column)\n",
        "    column = re.sub('/', ' ', column)\n",
        "    column = re.sub(\"'\", '', column)\n",
        "    column = re.sub(\",\", '', column)\n",
        "    column = re.sub(\":\", ' ', column)\n",
        "    column = re.sub('  +', ' ', column)\n",
        "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
        "    if not column:\n",
        "        column = None\n",
        "    return column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bsgYWum3u3A"
      },
      "source": [
        "def readData(filename):\n",
        "    \"\"\"\n",
        "    Read in our data from a CSV file and create a dictionary of records,\n",
        "    where the key is a unique record ID.\n",
        "    \"\"\"\n",
        "\n",
        "    data_d = {}\n",
        "\n",
        "    with open(filename) as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for i, row in enumerate(reader):\n",
        "            clean_row = dict([(k, preProcess(v)) for (k, v) in row.items()])\n",
        "            data_d[filename + str(i)] = dict(clean_row)\n",
        "\n",
        "    return data_d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4z8wwNq3u3A",
        "outputId": "830eaf8a-2663-4acb-cc17-6886efc5c42a"
      },
      "source": [
        "# ## Setup\n",
        "file_path = '/Users/gizelleguerra/Documents/fedex_cip/dedupe_NC_exp2/'\n",
        "output_file = 'NCdd2_output_it0.csv'\n",
        "settings_file = 'NCdd2_learned_settings'\n",
        "training_file = 'NCdd2_training.json'\n",
        "\n",
        "left_file = 'nc0_clean.csv'\n",
        "right_file = 'nc1_clean.csv'\n",
        "\n",
        "print('importing data ...')\n",
        "data_1 = readData(file_path + left_file)\n",
        "data_2 = readData(file_path + right_file)\n",
        "\n",
        "#def descriptions():\n",
        "#    for dataset in (data_1, data_2):\n",
        "#        for record in dataset.values():\n",
        "#            yield record['description']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "importing data ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSS828Pm3u3B",
        "outputId": "b6267925-58ce-4417-9cca-0a7b87f7707c"
      },
      "source": [
        "# ## Training\n",
        "\n",
        "if os.path.exists(settings_file):\n",
        "    print('reading from', settings_file)\n",
        "    with open(settings_file, 'rb') as sf:\n",
        "        linker = dedupe.StaticRecordLink(sf)\n",
        "\n",
        "else:\n",
        "    # Define the fields the linker will pay attention to\n",
        "    fields = [\n",
        "        {'field': 'givenname', 'type': 'Name'},\n",
        "        {'field': 'surname', 'type': 'Name'},\n",
        "        {'field': 'suburb', 'type': 'String'},\n",
        "        {'field': 'postcode', 'type': 'String', 'has missing': True},\n",
        "        ]\n",
        "    \n",
        "    # Create a new linker object and pass our data model to it.\n",
        "    linker = dedupe.RecordLink(fields)\n",
        "\n",
        "    # If we have training data saved from a previous run of linker,\n",
        "    # look for it an load it in.\n",
        "    # __Note:__ if you want to train from scratch, delete the training_file\n",
        "    if os.path.exists(training_file):\n",
        "        print('reading labeled examples from ', training_file)\n",
        "        with open(training_file) as tf:\n",
        "            linker.prepare_training(data_1,\n",
        "                                    data_2,\n",
        "                                    training_file=tf,\n",
        "                                    sample_size=15000)\n",
        "    else:\n",
        "        linker.prepare_training(data_1, data_2, sample_size=15000)\n",
        "\n",
        "    # ## Active learning\n",
        "    # Dedupe will find the next pair of records\n",
        "    # it is least certain about and ask you to label them as matches\n",
        "    # or not.\n",
        "    # use 'y', 'n' and 'u' keys to flag duplicates\n",
        "    # press 'f' when you are finished\n",
        "    print('starting active labeling...')\n",
        "\n",
        "    dedupe.console_label(linker)\n",
        "\n",
        "    linker.train()\n",
        "\n",
        "    # When finished, save our training away to disk\n",
        "    with open(training_file, 'w') as tf:\n",
        "        linker.write_training(tf)\n",
        "\n",
        "    # Save our weights and predicates to disk.  If the settings file\n",
        "    # exists, we will skip all the training and learning next time we run\n",
        "    # this file.\n",
        "    with open(settings_file, 'wb') as sf:\n",
        "        linker.write_settings(sf)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:dedupe.canopy_index:Removing stop word ar\n",
            "INFO:dedupe.canopy_index:Removing stop word an\n",
            "INFO:dedupe.canopy_index:Removing stop word er\n",
            "INFO:dedupe.canopy_index:Removing stop word an\n",
            "INFO:dedupe.canopy_index:Removing stop word on\n",
            "INFO:dedupe.canopy_index:Removing stop word re\n",
            "INFO:dedupe.canopy_index:Removing stop word ar\n",
            "INFO:dedupe.canopy_index:Removing stop word ma\n",
            "INFO:dedupe.canopy_index:Removing stop word el\n",
            "INFO:dedupe.canopy_index:Removing stop word le\n",
            "INFO:dedupe.canopy_index:Removing stop word in\n",
            "INFO:dedupe.canopy_index:Removing stop word en\n",
            "INFO:dedupe.canopy_index:Removing stop word er\n",
            "INFO:dedupe.canopy_index:Removing stop word an\n",
            "INFO:dedupe.canopy_index:Removing stop word on\n",
            "INFO:dedupe.canopy_index:Removing stop word ar\n",
            "INFO:dedupe.canopy_index:Removing stop word el\n",
            "INFO:dedupe.canopy_index:Removing stop word le\n",
            "INFO:dedupe.canopy_index:Removing stop word in\n",
            "INFO:dedupe.canopy_index:Removing stop word ha\n",
            "INFO:dedupe.canopy_index:Removing stop word ra\n",
            "INFO:dedupe.canopy_index:Removing stop word ne\n",
            "INFO:dedupe.canopy_index:Removing stop word ro\n",
            "INFO:dedupe.canopy_index:Removing stop word ch\n",
            "INFO:dedupe.canopy_index:Removing stop word ll\n",
            "INFO:dedupe.canopy_index:Removing stop word al\n",
            "INFO:dedupe.canopy_index:Removing stop word ri\n",
            "INFO:dedupe.canopy_index:Removing stop word or\n",
            "INFO:dedupe.canopy_index:Removing stop word st\n",
            "INFO:dedupe.canopy_index:Removing stop word ma\n",
            "INFO:dedupe.canopy_index:Removing stop word ll\n",
            "INFO:dedupe.canopy_index:Removing stop word en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwK-aKC23u3C",
        "outputId": "fa254226-18c0-4889-81a6-96c629fde009"
      },
      "source": [
        "# ## Blocking\n",
        "# ## Clustering\n",
        "\n",
        "# Find the threshold that will maximize a weighted average of our\n",
        "# precision and recall.  When we set the recall weight to 2, we are\n",
        "# saying we care twice as much about recall as we do precision.\n",
        "#\n",
        "# If we had more data, we would not pass in all the blocked data into\n",
        "# this function but a representative sample.\n",
        "\n",
        "print('clustering...')\n",
        "linked_records = linker.join(data_1, data_2, 0.0)\n",
        "\n",
        "print('# duplicate sets', len(linked_records))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "clustering...\n",
            "# duplicate sets 173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBHUt1bQ3u3C"
      },
      "source": [
        "## ## Writing Results\n",
        "# Write our original data back out to a CSV with a new column called\n",
        "# 'Cluster ID' which indicates which records refer to each other.\n",
        "\n",
        "cluster_membership = {}\n",
        "for cluster_id, (cluster, score) in enumerate(linked_records):\n",
        "    for record_id in cluster:\n",
        "        cluster_membership[record_id] = {'Cluster ID': cluster_id,\n",
        "                                         'Link Score': score}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vopRyra3u3D"
      },
      "source": [
        "out_f = file_path + output_file\n",
        "right_f = file_path + left_file\n",
        "left_f = file_path + right_file\n",
        "\n",
        "with open(out_f, 'w') as f:\n",
        "\n",
        "    header_unwritten = True\n",
        "\n",
        "    for fileno, filename in enumerate((left_f, right_f)):\n",
        "        with open(filename) as f_input:\n",
        "            reader = csv.DictReader(f_input)\n",
        "\n",
        "            if header_unwritten:\n",
        "\n",
        "                fieldnames = (['Cluster ID', 'Link Score', 'source file'] +\n",
        "                                reader.fieldnames)\n",
        "\n",
        "                writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "\n",
        "                header_unwritten = False\n",
        "\n",
        "            for row_id, row in enumerate(reader):\n",
        "\n",
        "                record_id = filename + str(row_id)\n",
        "                cluster_details = cluster_membership.get(record_id, {})\n",
        "                row['source file'] = fileno\n",
        "                row.update(cluster_details)\n",
        "\n",
        "                writer.writerow(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8aEpPgt3u3D"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2-UIoho3u3D"
      },
      "source": [
        "def evaluateDuplicates(found_dupes, true_dupes):\n",
        "    true_positives = found_dupes.intersection(true_dupes)\n",
        "    false_positives = found_dupes.difference(true_dupes)\n",
        "    uncovered_dupes = true_dupes.difference(found_dupes)\n",
        "\n",
        "    print('found duplicate')\n",
        "    print(len(found_dupes))\n",
        "\n",
        "    print('precision')\n",
        "    print(1 - len(false_positives) / float(len(found_dupes)))\n",
        "\n",
        "    print('recall')\n",
        "    print(len(true_positives) / float(len(true_dupes)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e1Lp8nt3u3E"
      },
      "source": [
        "def linkPairs(filename, rowname) :\n",
        "    link_d = {}\n",
        "\n",
        "    with open(filename) as f:\n",
        "        reader = csv.DictReader(f, delimiter=',', quotechar='\"')\n",
        "        for i, row in enumerate(reader):\n",
        "            source_file, link_id = row['source file'], row[rowname]\n",
        "            if link_id:\n",
        "                if link_id not in link_d:\n",
        "                    link_d[link_id] = collections.defaultdict(list)\n",
        "\n",
        "                link_d[link_id][source_file].append(i)\n",
        "\n",
        "    link_s = set()\n",
        "\n",
        "    for members in link_d.values():\n",
        "        for pair in itertools.product(*members.values()):\n",
        "            link_s.add(frozenset(pair))\n",
        "\n",
        "    return link_s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n33uEF73u3E",
        "outputId": "1934f4b5-35f9-4338-bd79-c5513c14844b"
      },
      "source": [
        "clusters = 'NCdd2_output_it0.csv'\n",
        "clusters_fp = file_path + clusters\n",
        "\n",
        "true_dupes = linkPairs(clusters_fp, 'unique_id')\n",
        "test_dupes = linkPairs(clusters_fp, 'Cluster ID')\n",
        "\n",
        "evaluateDuplicates(test_dupes, true_dupes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "found duplicate\n",
            "173\n",
            "precision\n",
            "0.7283236994219653\n",
            "recall\n",
            "0.11485870556061988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7M1YtQN3u3E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}